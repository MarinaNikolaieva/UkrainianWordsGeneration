{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7PWXQ1ENrGEv"
      },
      "outputs": [],
      "source": [
        "vowels = ['а', 'е', 'є', 'и', 'і', 'ї', 'о', 'у', 'ю', 'я']\n",
        "consonants = ['б', 'в', 'г', 'ґ', 'д', 'й', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ']\n",
        "soften = 'ь'\n",
        "preSoften = ['д', 'т', 'з', 'с', 'ц', 'л', 'н']\n",
        "#For now, there will be no apostrophe\n",
        "\n",
        "#The formula for the number of skip-grams will be wordLetterNumber^2 * magicX\n",
        "wordLetterNumber = 10\n",
        "magicX = 0.1\n",
        "#Probability of next letter to be a vowel / consonant\n",
        "vowelProb = 0.5\n",
        "consonantProb = 0.5\n",
        "softenProb = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the random-formed word from vowels and consonants\n",
        "#This is a COMPLETE RANDOM selector. Markov chain or something similar can give better letter prediction\n",
        "import random\n",
        "\n",
        "def GenerateWord(wordLetterNumber, vowelProb, consonantProb, softenProb):\n",
        "  word = \"\"\n",
        "  prevLetter = ''\n",
        "  curLetter = ''\n",
        "  for i in range(0, wordLetterNumber):\n",
        "    #The probability of the next letter to be a vowel. If it's greater than the vowelProb, the letter is a consonant\n",
        "    probability = random.random()\n",
        "    if probability <= vowelProb:\n",
        "      word = word + random.choice(vowels)\n",
        "      if i == 0:\n",
        "        prevLetter = 'v'\n",
        "        curLetter = 'v'\n",
        "      else:\n",
        "        prevLetter = curLetter\n",
        "        curLetter = 'v'\n",
        "    else:\n",
        "      #If we got a consonant, we may, with some prob, get a softened one\n",
        "      randConsonant = random.choice(consonants)\n",
        "      softProbability = random.random()\n",
        "      if softProbability <= softenProb and randConsonant in preSoften:\n",
        "        word = word + randConsonant + soften\n",
        "      else:\n",
        "        word = word + randConsonant\n",
        "      if i == 0:\n",
        "        prevLetter = 'c'\n",
        "        curLetter = 'c'\n",
        "      else:\n",
        "        prevLetter = curLetter\n",
        "        curLetter = 'c'\n",
        "    if prevLetter == curLetter and i != 0:\n",
        "      if prevLetter == 'v':\n",
        "        probToShare = vowelProb / 2\n",
        "        vowelProb = vowelProb - probToShare\n",
        "        consonantProb = consonantProb + probToShare\n",
        "      elif prevLetter == 'c':\n",
        "        probToShare = consonantProb / 2\n",
        "        vowelProb = vowelProb + probToShare\n",
        "        consonantProb = consonantProb - probToShare\n",
        "    print(\"W: \" + word + \" NextV: \" + str(vowelProb) + \" NextC: \" + str(consonantProb))\n",
        "    if len(word) >= 10:\n",
        "      break\n",
        "  return word\n",
        "\n",
        "print(GenerateWord(wordLetterNumber, vowelProb, consonantProb, softenProb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS7kY_Aj_S3j",
        "outputId": "52359cc3-3db2-46cc-b104-e3d1282da1d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: е NextV: 0.5 NextC: 0.5\n",
            "W: ед NextV: 0.5 NextC: 0.5\n",
            "W: еді NextV: 0.5 NextC: 0.5\n",
            "W: едіб NextV: 0.5 NextC: 0.5\n",
            "W: едібо NextV: 0.5 NextC: 0.5\n",
            "W: едібоу NextV: 0.25 NextC: 0.75\n",
            "W: едібоум NextV: 0.25 NextC: 0.75\n",
            "W: едібоумі NextV: 0.25 NextC: 0.75\n",
            "W: едібоуміх NextV: 0.25 NextC: 0.75\n",
            "W: едібоуміхж NextV: 0.625 NextC: 0.375\n",
            "едібоуміхж\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to Google Drive\n",
        "#https://github.com/lang-uk/ukrainian-word-stress-dictionary/\n",
        "#https://pypi.org/project/markov-word-generator/\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#Install the word generation library (core algorithm is the Markov chain)\n",
        "!pip install markov-word-generator\n",
        "!pip install unidecode\n",
        "import unidecode\n",
        "from markov_word_generator import MarkovWordGenerator\n",
        "\n",
        "#And generate the word similar to the real Ukrainian word\n",
        "#The dictionary is my custom file found by the first link, cleared from stress symbols and duplicates\n",
        "generator = MarkovWordGenerator(\n",
        "\tmarkov_length=4,\n",
        "\tdictionary_filename='/content/gdrive/My Drive/Kaggle/ukr-word-dict.txt'\n",
        ")\n",
        "print(generator.generate_word())"
      ],
      "metadata": {
        "id": "NcOhJ_zQn_88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edd5343-a108-4294-9def-8e91b4e3b05a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Collecting markov-word-generator\n",
            "  Downloading markov_word_generator-0.3-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: markov-word-generator\n",
            "Successfully installed markov-word-generator-0.3\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.7\n",
            "хромочками\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "wordNumber = 10\n",
        "\n",
        "#Generate some words for further analysis\n",
        "while len(words) < wordNumber:\n",
        "  word = generator.generate_word()\n",
        "  if len(word) > 5:\n",
        "    words.append(generator.generate_word())\n",
        "\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "1Glv3Zc3YXko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6834150-c1f5-4692-ee6b-1dc678fe9a86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "дере\n",
            "переряджуймо\n",
            "спеціалозабудкоокоссорельнім\n",
            "шубованим\n",
            "декламним\n",
            "видай\n",
            "чкалачна\n",
            "відеоцинко\n",
            "мишаних\n",
            "зашиваної\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgramFormer(word, index, substringLength):\n",
        "  skipgram = word[:index]\n",
        "  skipgram = skipgram + word[index + substringLength:]\n",
        "  return skipgram\n",
        "\n",
        "print(skipgramFormer(\"ігровий\", 2, 3))"
      ],
      "metadata": {
        "id": "nRireIoDwmcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545599d3-3a3f-483f-e48b-6e57c2b45b46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ігий\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"урисушабив\"\n",
        "\n",
        "#Generating skipgrams\n",
        "#Skipgram array\n",
        "skipgrams = []\n",
        "skipgramFstRemoved = []\n",
        "skipgramSndRemoved = []\n",
        "#Keep the lengths of all parts - useful when restoring the word\n",
        "skipgramLengths = []\n",
        "\n",
        "#Choose a random word to process\n",
        "word = random.choice(words)\n",
        "skipgramNumber = int(len(word)**2 * magicX)\n",
        "print(skipgramNumber)\n",
        "print(word)\n",
        "for i in range (0, skipgramNumber):\n",
        "  #There can be either one or two strips\n",
        "  cutNumber = random.randint(1, 3)\n",
        "  #The max substring length = length of word - 5\n",
        "  maxSubstringLen = len(word) - 5\n",
        "  if maxSubstringLen > 2:\n",
        "    if cutNumber == 1:\n",
        "      #For the only cut, we can strip any part of the word\n",
        "      curSubstringLen = random.randrange(1, maxSubstringLen + 1)\n",
        "      index = random.randrange(0, len(word) - curSubstringLen + 1)\n",
        "      #Keep the lengths of all the word segments\n",
        "      lengths = []\n",
        "      lengths.append(index)\n",
        "      lengths.append(curSubstringLen)\n",
        "      lengths.append(len(word) - curSubstringLen - index)\n",
        "      #And put everything to the corresponding arrays\n",
        "      skipgramFstRemoved.append(word[index : (index + curSubstringLen)])\n",
        "      skipgramSndRemoved.append(\"\")\n",
        "      skipgrams.append(skipgramFormer(word, index, curSubstringLen))\n",
        "      skipgramLengths.append(lengths)\n",
        "    else:\n",
        "      #For two cuts, we need to strip two different parts of the word\n",
        "      firstSubstrLen = random.randrange(1, maxSubstringLen - 1)\n",
        "      secondSubstrLen = random.randrange(1, maxSubstringLen - firstSubstrLen + 1)\n",
        "      #There must be at least one letter between the cuts\n",
        "      lengthLeft = len(word) - firstSubstrLen - secondSubstrLen\n",
        "      firstLeftOver = random.randrange(0, lengthLeft - 1)\n",
        "      secondLeftOver = random.randrange(1, lengthLeft - firstLeftOver)\n",
        "      thirdLeftOver = lengthLeft - firstLeftOver - secondLeftOver\n",
        "      #When done, put the LeftOvers into the string - this is the Skipgram\n",
        "      firstSkipgramPart = word[: firstLeftOver]\n",
        "      secondSkipgramPart = word[firstLeftOver + firstSubstrLen : firstLeftOver + firstSubstrLen + secondLeftOver]\n",
        "      thirdSkipgramPart = word[firstLeftOver + firstSubstrLen + secondLeftOver + secondSubstrLen :]\n",
        "      skipgram = firstSkipgramPart + secondSkipgramPart + thirdSkipgramPart\n",
        "      #And keep the removed parts, too\n",
        "      firstRemoved = word[firstLeftOver : firstLeftOver + firstSubstrLen]\n",
        "      secondRemoved = word[firstLeftOver + firstSubstrLen + secondLeftOver : firstLeftOver + firstSubstrLen + secondLeftOver + secondSubstrLen]\n",
        "      #Keep the lengths\n",
        "      lengths = []\n",
        "      lengths.append(firstLeftOver)\n",
        "      lengths.append(firstSubstrLen)\n",
        "      lengths.append(secondLeftOver)\n",
        "      lengths.append(secondSubstrLen)\n",
        "      lengths.append(thirdLeftOver)\n",
        "      #Put everything to correct arrays\n",
        "      skipgramFstRemoved.append(firstRemoved)\n",
        "      skipgramSndRemoved.append(secondRemoved)\n",
        "      skipgrams.append(skipgram)\n",
        "      skipgramLengths.append(lengths)\n",
        "\n",
        "for i in range(0, len(skipgrams)):\n",
        "  print(\"Sk: \" + skipgrams[i] + \" ; Fst: \" + skipgramFstRemoved[i] + \" ; Snd: \" + skipgramSndRemoved[i])"
      ],
      "metadata": {
        "id": "IBNrPS_M2vur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1137f0e-f893-44b2-946e-a2333870d039"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "чкалачна\n",
            "Sk: чкачна ; Fst: ал ; Snd: \n",
            "Sk: чклчна ; Fst: а ; Snd: а\n",
            "Sk: чкаача ; Fst: л ; Snd: н\n",
            "Sk: чкачна ; Fst: ла ; Snd: \n",
            "Sk: чкачна ; Fst: ал ; Snd: \n",
            "Sk: чкалчна ; Fst: а ; Snd: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read my dictionary file as a list of words\n",
        "openedFile = open('/content/gdrive/My Drive/Kaggle/ukr-word-dict.txt', \"r\")\n",
        "wordList = openedFile.read().split(\"\\n\")\n",
        "openedFile.close()\n",
        "print(wordList[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdSykXwWslJE",
        "outputId": "721f7143-0023-4960-a25b-8e6469429b8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['а', 'а вже ж', 'а все ж таки', 'а все-таки', 'а капела', 'а ні', 'а то', 'а то ж', 'а як же', 'а-конто']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find words similar to a part of the word\n",
        "#https://pypi.org/project/python-Levenshtein/\n",
        "!pip install python-Levenshtein\n",
        "import Levenshtein\n",
        "import difflib\n",
        "\n",
        "def LevensteinSimilarity(word, dictionaryWordList, similarsNum):\n",
        "  #Return -1 for empty word or for the word with nothing similar to find\n",
        "  if len(word) == 0:\n",
        "    return -1\n",
        "  lenLimitDown = 0\n",
        "  if len(word) > 2:\n",
        "    lenLimitDown = len(word) - 1\n",
        "  lenLimitUp = len(word) + 3\n",
        "  #The list of words should be filtered by length\n",
        "  tempWordList = list([x for x in dictionaryWordList if len(x) >= lenLimitDown and len(x) <= lenLimitUp])\n",
        "  matches = difflib.get_close_matches(word, tempWordList, similarsNum)\n",
        "  dictionary = {}\n",
        "  for mat in matches:\n",
        "    dictionary[mat] = -1\n",
        "  #Weights are (insertion, deletion, substitution)\n",
        "  for matche in matches:\n",
        "    dist = Levenshtein.distance(word, matche, weights=(1,1,1))\n",
        "    dictionary[matche] = dist\n",
        "  newDict = dict(sorted(dictionary.items(), key=lambda x: (x[1],x[0])))\n",
        "  #for key, value in newDict.items():\n",
        "  #  print(str(key) + \" \" + str(value))\n",
        "  return list(newDict.items())[0][1]"
      ],
      "metadata": {
        "id": "-U4aHeW_mn5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee92de8-cc74-4d4f-ff5f-a605c6957b48"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now check each part of the word - the skipgram and both cut-offs\n",
        "#I'll set the max Levenshtein distance to 2 for now\n",
        "maxLevenDist = 2\n",
        "similarsNum = 15\n",
        "\n",
        "goodWords = []\n",
        "for i in range(0, len(skipgrams)):\n",
        "  levenSkipgram = LevensteinSimilarity(skipgrams[i], wordList, similarsNum)\n",
        "  levenFstRemoved = LevensteinSimilarity(skipgramFstRemoved[i], wordList, similarsNum)\n",
        "  levenSndRemoved = LevensteinSimilarity(skipgramSndRemoved[i], wordList, similarsNum)\n",
        "  print(str(levenSkipgram) + \" \" + str(levenFstRemoved) + \" \" + str(levenSndRemoved))\n",
        "  #Now check if fits\n",
        "  if levenSkipgram <= maxLevenDist and levenFstRemoved <= maxLevenDist and levenFstRemoved != -1 and levenSndRemoved <= maxLevenDist and levenSndRemoved != -1:\n",
        "    #If all three fit, the word is good, and we need to put it back together\n",
        "    restoreWord = \"\"\n",
        "    lengths = skipgramLengths[i]\n",
        "    restoreWord = restoreWord + skipgrams[i][:lengths[0]]\n",
        "    restoreWord = restoreWord + skipgramFstRemoved[i]\n",
        "    if skipgramSndRemoved[i] == \"\":\n",
        "      restoreWord = restoreWord + skipgrams[i][lengths[0]:]\n",
        "    else:\n",
        "      restoreWord = restoreWord + skipgrams[i][lengths[0] : lengths[0] + lengths[2]]\n",
        "      restoreWord = restoreWord + skipgramSndRemoved[i]\n",
        "      restoreWord = restoreWord + skipgrams[i][lengths[0] + lengths[2] :]\n",
        "    if restoreWord not in goodWords:\n",
        "      goodWords.append(restoreWord)\n",
        "  elif levenSkipgram <= maxLevenDist and levenFstRemoved <= maxLevenDist and levenFstRemoved != -1:\n",
        "    #Second part doesn't fit, so we put together only the first and main part\n",
        "    restoreWord = \"\"\n",
        "    lengths = skipgramLengths[i]\n",
        "    restoreWord = restoreWord + skipgrams[i][:lengths[0]]\n",
        "    restoreWord = restoreWord + skipgramFstRemoved[i]\n",
        "    restoreWord = restoreWord + skipgrams[i][lengths[0]:]\n",
        "    if restoreWord not in goodWords:\n",
        "      goodWords.append(restoreWord)\n",
        "  elif levenSkipgram <= maxLevenDist and levenSndRemoved <= maxLevenDist and levenSndRemoved != -1:\n",
        "    #First part doesn't fit, so we put together the main and second part\n",
        "    restoreWord = \"\"\n",
        "    lengths = skipgramLengths[i]\n",
        "    restoreWord = restoreWord + skipgrams[i][: lengths[0] + lengths[2]]\n",
        "    restoreWord = restoreWord + skipgramSndRemoved[i]\n",
        "    restoreWord = restoreWord + skipgrams[i][lengths[0] + lengths[2] :]\n",
        "    if restoreWord not in goodWords:\n",
        "      goodWords.append(restoreWord)\n",
        "  elif levenFstRemoved <= maxLevenDist and levenFstRemoved != -1 and levenSndRemoved <= maxLevenDist and levenSndRemoved != -1:\n",
        "    #Main part doesn't fit, so we put together the first and second removed parts\n",
        "    restoreWord = skipgramFstRemoved[i] + skipgramSndRemoved[i]\n",
        "    if restoreWord not in goodWords:\n",
        "      goodWords.append(restoreWord)\n",
        "  elif levenSkipgram <= maxLevenDist:\n",
        "    #Only the main skipgram fits\n",
        "    if skipgrams[i] not in goodWords:\n",
        "      goodWords.append(skipgrams[i])\n",
        "  elif levenFstRemoved <= maxLevenDist and levenFstRemoved != -1:\n",
        "    #Only the first removed part fits\n",
        "    if skipgramFstRemoved[i] not in goodWords:\n",
        "      goodWords.append(skipgramFstRemoved[i])\n",
        "  elif levenSndRemoved <= maxLevenDist and levenSndRemoved != -1:\n",
        "    #Only the second removed part fits\n",
        "    if skipgramSndRemoved[i] not in goodWords:\n",
        "      goodWords.append(skipgramSndRemoved[i])\n",
        "\n",
        "for word in goodWords:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc7fLhpFnfiP",
        "outputId": "f8eaae47-3e0c-4ff3-8bc8-39f969b0b1bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 1 -1\n",
            "2 0 0\n",
            "2 1 1\n",
            "2 1 -1\n",
            "2 1 -1\n",
            "2 0 -1\n",
            "чкалачна\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "word = \"abcdefghijklmno\"\n",
        "lenWord = 15\n",
        "maxSubstringLen = lenWord - 5\n",
        "lengths = []\n",
        "firstSubstrLen = random.randrange(0, maxSubstringLen - 1)\n",
        "secondSubstrLen = random.randrange(1, maxSubstringLen - firstSubstrLen + 1)\n",
        "lengthLeft = lenWord - firstSubstrLen - secondSubstrLen\n",
        "firstLeftOver = random.randrange(0, lengthLeft - 1)\n",
        "secondLeftOver = random.randrange(1, lengthLeft - firstLeftOver)\n",
        "thirdLeftOver = lengthLeft - firstLeftOver - secondLeftOver\n",
        "print(word)\n",
        "print(firstSubstrLen + secondSubstrLen + firstLeftOver + secondLeftOver + thirdLeftOver)\n",
        "print(firstSubstrLen)\n",
        "print(secondSubstrLen)\n",
        "print(firstLeftOver)\n",
        "print(secondLeftOver)\n",
        "print(thirdLeftOver)\n",
        "lengths.append(firstLeftOver)\n",
        "lengths.append(firstSubstrLen)\n",
        "lengths.append(secondLeftOver)\n",
        "lengths.append(secondSubstrLen)\n",
        "lengths.append(thirdLeftOver)\n",
        "\n",
        "firstSkipgramPart = word[: firstLeftOver]\n",
        "secondSkipgramPart = word[firstLeftOver + firstSubstrLen : firstLeftOver + firstSubstrLen + secondLeftOver]\n",
        "thirdSkipgramPart = word[firstLeftOver + firstSubstrLen + secondLeftOver + secondSubstrLen :]\n",
        "skipgram = firstSkipgramPart + secondSkipgramPart + thirdSkipgramPart\n",
        "firstRemoved = word[firstLeftOver : firstLeftOver + firstSubstrLen]\n",
        "secondRemoved = word[firstLeftOver + firstSubstrLen + secondLeftOver : firstLeftOver + firstSubstrLen + secondLeftOver + secondSubstrLen]\n",
        "print(firstSkipgramPart)\n",
        "print(secondSkipgramPart)\n",
        "print(thirdSkipgramPart)\n",
        "\n",
        "print(\"Skipgram: \" + skipgram)\n",
        "print(\"First removed part: \" + firstRemoved)\n",
        "print(\"Second removed part: \" + secondRemoved)\n",
        "\n",
        "print(\"\\n\")\n",
        "#Need to check the put-back mechanism as well\n",
        "#If all three fit, the word is good, and we need to put it back together\n",
        "restoreWord = \"\"\n",
        "restoreWord = restoreWord + skipgram[:lengths[0]]\n",
        "restoreWord = restoreWord + firstRemoved\n",
        "if secondRemoved == \"\":\n",
        "  restoreWord = restoreWord + skipgram[lengths[0]:]\n",
        "else:\n",
        "  restoreWord = restoreWord + skipgram[lengths[0] : lengths[0] + lengths[2]]\n",
        "  restoreWord = restoreWord + secondRemoved\n",
        "  restoreWord = restoreWord + skipgram[lengths[0] + lengths[2] :]\n",
        "print(\"All three: \" + restoreWord)\n",
        "#Second part doesn't fit, so we put together only the first and main part\n",
        "restoreWord = \"\"\n",
        "restoreWord = restoreWord + skipgram[:lengths[0]]\n",
        "restoreWord = restoreWord + firstRemoved\n",
        "restoreWord = restoreWord + skipgram[lengths[0]:]\n",
        "print(\"Main and first: \" + restoreWord)\n",
        "#First part doesn't fit, so we put together the main and second part\n",
        "restoreWord = \"\"\n",
        "restoreWord = restoreWord + skipgram[: lengths[0] + lengths[2]]\n",
        "restoreWord = restoreWord + secondRemoved\n",
        "restoreWord = restoreWord + skipgram[lengths[0] + lengths[2] :]\n",
        "print(\"Main and second: \" + restoreWord)\n",
        "#Main part doesn't fit, so we put together the first and second removed parts\n",
        "restoreWord = firstRemoved + secondRemoved\n",
        "print(\"First and second: \" + restoreWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVOYQ616oxjd",
        "outputId": "dcf02ecb-86fc-4351-d356-edb237819505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklmno\n",
            "15\n",
            "5\n",
            "1\n",
            "5\n",
            "1\n",
            "3\n",
            "abcde\n",
            "k\n",
            "mno\n",
            "Skipgram: abcdekmno\n",
            "First removed part: fghij\n",
            "Second removed part: l\n",
            "\n",
            "\n",
            "All three: abcdefghijklmno\n",
            "Main and first: abcdefghijkmno\n",
            "Main and second: abcdeklmno\n",
            "First and second: fghijl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "word = \"доярівкою\"\n",
        "Skipg = \"дяріою\"\n",
        "Fst = \"о\"\n",
        "Snd = \"вк\"\n",
        "levenSkipgram = LevensteinSimilarity(Skipg, wordList, 15)\n",
        "print(\"\\n\")\n",
        "levenFstRemoved = LevensteinSimilarity(Fst, wordList, 15)\n",
        "print(\"\\n\")\n",
        "levenSndRemoved = LevensteinSimilarity(Snd, wordList, 15)\n",
        "print(str(levenSkipgram) + \" \" + str(levenFstRemoved) + \" \" + str(levenSndRemoved))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Y8_r5JgG8a",
        "outputId": "873ce3c2-0795-4965-f4f0-8a27bbe2914b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "дояркою 2\n",
            "дякою 2\n",
            "дірою 2\n",
            "яркою 2\n",
            "ярішою 2\n",
            "дріадою 3\n",
            "дрібкою 3\n",
            "дрібною 3\n",
            "дівою 3\n",
            "одрою 3\n",
            "різою 3\n",
            "рікою 3\n",
            "ріпою 3\n",
            "яркішою 3\n",
            "яріло 3\n",
            "\n",
            "\n",
            "о 0\n",
            "ов 1\n",
            "од 1\n",
            "ож 1\n",
            "ой 1\n",
            "ок 1\n",
            "ом 1\n",
            "он 1\n",
            "ос 1\n",
            "от 1\n",
            "ох 1\n",
            "по 1\n",
            "то 1\n",
            "хо 1\n",
            "що 1\n",
            "\n",
            "\n",
            "вак 1\n",
            "вік 1\n",
            "звик 2\n",
            "звук 2\n",
            "звяк 2\n",
            "мовк 2\n",
            "товк 2\n",
            "увік 2\n",
            "чвак 2\n",
            "шовк 2\n",
            "явка 2\n",
            "явки 2\n",
            "явко 2\n",
            "явку 2\n",
            "явок 2\n",
            "2 0 1\n"
          ]
        }
      ]
    }
  ]
}